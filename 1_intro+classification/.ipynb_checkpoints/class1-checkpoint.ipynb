{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ø–æ—Å–ª–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞—Å—Ç–∞–ª–æ –≤—Ä–µ–º—è –ø—Ä–∞–∫—Ç–∏–∫–∏ üôå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.18.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.8/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "# ! pip3 list\n",
    "! pip3 install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (–ù–∞–∏–≤–Ω—ã–π –ë–∞–π–µ—Å)\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–∏—Å–µ–º –Ω–∞ \"—Å–ø–∞–º/–Ω–µ —Å–ø–∞–º\". –ù–∞–ø—Ä–∏–º–µ—Ä, –Ω–∞ [—ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö](https://www.kaggle.com/uciml/sms-spam-collection-dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —è—á–µ–π–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\n",
    "\n",
    "# from IPython.display import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# —á–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "\n",
    "data = pd.read_csv('spam.csv', encoding='latin-1') # –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫—É\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ—á–∏—Å—Ç–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º\n",
    "\n",
    "data = data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
    "\n",
    "data = data.rename(columns={\"v1\": \"spam\", \"v2\": \"content\"})\n",
    "\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¥–∞–≤–∞–π—Ç–µ –ø–æ—Å—á–∏—Ç–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∞–º–∞/–Ω–µ —Å–ø–∞–º–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes =pd.value_counts(data[\"spam\"])\n",
    "\n",
    "classes.plot(kind = 'pie',  autopct='%0.f%%') \n",
    "# –ø—Ä–æ string formatting https://realpython.com/python-string-formatting/\n",
    "\n",
    "plt.title(\"Classes distribution\")\n",
    "\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö**\n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ –Ω–∞–π–¥–µ–º —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö (–æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø–æ–¥–∫–æ—Ä–ø—É—Å–∞ —Å–ø–∞–º–∞ –∏ –Ω–µ-—Å–ø–∞–º–∞). –î–ª—è –±—É–¥—É—â–µ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞, —Å–ª–æ–≤–∞ –±—É–¥—É—Ç –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ (features)\n",
    "\n",
    "–î–ª—è —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º Counter –∏–∑ collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(\" \".join(data[data['spam']=='ham'][\"content\"]).split()).most_common(50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –∑–∞–±–µ—Ä–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –ø–æ —É—Å–ª–æ–≤–∏—é, –∑–∞–¥–∂–æ–π–Ω–∏–º –≤ —Å—Ç—Ä–æ–∫—É, —Ä–∞–∑–æ–±—å–µ–º –ø–æ –ø—Ä–æ–±–µ–ª–∞–º, –≤–æ–∑—å–º–µ–º —Ç–æ–ø-20\n",
    "count1 = Counter(\" \".join(data[data['spam']=='ham'][\"content\"]).lower().split()).most_common(20) \n",
    "df1 = pd.DataFrame.from_dict(count1) # –Ω–æ–≤—ã–π –¥—Ñ\n",
    "df1 = df1.rename(columns={0: \"words in non-spam\", 1 : \"count\"})\n",
    "\n",
    "# —Ç–æ—Ç –∂–µ –Ω–∞–±–æ—Ä –æ–ø–µ—Ä–∞—Ü–∏–π –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ –ø–æ–¥–∫–æ—Ä–ø—É—Å–∞\n",
    "count2 = Counter(\" \".join(data[data['spam']=='spam'][\"content\"]).lower().split()).most_common(20)\n",
    "df2 = pd.DataFrame.from_dict(count2)\n",
    "df2 = df2.rename(columns={0: \"words in spam\", 1 : \"count\"})\n",
    "\n",
    "display(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–æ—á–µ–Ω—å –º–Ω–æ–≥–æ \"—à—É–º–∞\", –¥–∞–≤–∞–π—Ç–µ –∏–∑–±–∞–≤–∏–º—Å—è –æ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–µ–º [—ç—Ç—É —Ñ—É–Ω–∫—Ü–∏—é](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) –∏–∑ sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = feature_extraction.text.CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ø—Ä–∏–º–µ—Ä –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–π –º–∞—Ç—Ä–∏—Ü–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.','Is this my the second document?']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "display(X.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –∏–¥–µ–º –¥–∞–ª—å—à–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = f.fit_transform(data[\"content\"])\n",
    "# print(f.get_feature_names()) # –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "\n",
    "display(X.shape)\n",
    "display(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–º–µ—Ç–æ–¥ ```.fit_transform()``` –≤—ã—É—á–∏–≤–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Ñ–æ—Ä–º–∞—Ç–∞ \"–¥–æ–∫—É–º–µ–Ω—Ç-—Ç–µ—Ä–º–∏–Ω\" –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "\n",
    "–°–µ–π—á–∞—Å —É –Ω–∞—Å 8404 –ø—Ä–∏–∑–Ω–∞–∫–∞ (–ø–æ —á–∏—Å–ª—É —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ). –ù–æ–≤—ã–π –ø—Ä–∏–∑–Ω–∞–∫  j  –≤ —Ä—è–¥—É i  —Ä–∞–≤–µ–Ω 1 –µ—Å–ª–∏ —Å–ª–æ–≤–æ j  –ø–æ—è–≤–ª—è–µ—Ç—Å—è –≤ —Ç–µ–∫—Å—Ç–µ i . –í –∏–Ω–æ–º —Å–ª—É—á–∞–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∞ = 0\n",
    "\n",
    "**–¥–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å:** –æ–∫–∞–∂–µ—Ç—Å—è –ª–∏ –ø–∏—Å—å–º–æ —Å–ø–∞–º–æ–º –∏–ª–∏ –Ω–µ —Å–ø–∞–º–æ–º\n",
    "\n",
    "–°–ª–µ–¥—É–µ—Ç –ø–æ–º–Ω–∏—Ç—å –æ –¥–≤—É—Ö –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö:\n",
    "\n",
    "- —Å–ø–∞–º-–ø–∏—Å—å–º–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ (False Negative)\n",
    "- –æ–±—ã—á–Ω–æ–µ –ø–∏—Å—å–º–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç—Å—è –∫–∞–∫ —Å–ø–∞–º (False positive).\n",
    "(–∫–∞–∫–æ–π –≤–∞—Ä–∏–∞–Ω—Ç —Ö—É–∂–µ?)\n",
    "\n",
    "–°–Ω–∞—á–∞–ª–∞, –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–º –∑–Ω–∞—á–µ–Ω–∏—è spam/ham –∫–∞–∫ —á–∏—Å–ª–æ–≤—ã–µ (1,0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞—Ç–µ–º —Ä–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫—É [—ç—Ç–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"spam\"]=data[\"spam\"].map({'spam':1,'ham':0}) # –≤ –ø–µ—Ä–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è 'spam' –≤ 1  –∏ 'ham' –≤ 0\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –Ω–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:\n",
    "- X_train - –∫—É—Å–æ—á–µ–∫ –º–∞—Ç—Ä–∏—Ü—ã \"–¥–æ–∫—É–º–µ–Ω—Ç/—Ç–µ—Ä–º–∏–Ω\", –∫–æ—Ç–æ—Ä—ã–π –º—ã \"–æ—Ç–¥–∞–¥–∏–º\" –∞–ª–≥–æ—Ä–∏—Ç–º—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "- X_test - –∫—É—Å–æ—á–µ–∫ –º–∞—Ç—Ä–∏—Ü—ã \"–¥–æ–∫—É–º–µ–Ω—Ç/—Ç–µ—Ä–º–∏–Ω\", –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å, –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ä–∞–±–æ—Ç–∞–ª \n",
    "- y_train - –∫—É—Å–æ—á–µ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π –º—ã –æ—Ç–¥–∞–¥–∏–º –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "- y_test -  –∫—É—Å–æ—á–µ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ —Å —Ç–µ–∫—Å—Ç–∞–º–∏, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—É–¥–µ–º –ø—Ä–æ–≤–µ—Ä—è—Ç—å, –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ä–∞–±–æ—Ç–∞–ª "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –Ω–æ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, data['spam'], \n",
    "                                                                    test_size=0.2, random_state=50)\n",
    "\n",
    "# print(X_train.shape, X_test.shape)\n",
    "sum(y_test == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –Ω–∞–∏–≤–Ω–æ–≥–æ –±–∞–π–µ—Å–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ, –º—ã –≤–æ–∑—å–º–µ–º [Multinomial NB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "\n",
    "[–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ Naive Bayes](https://nlp.stanford.edu/IR-book/pdf/13bayes.pdf)\n",
    "\n",
    "–¢–∞–∫ –∫–∞–∫ –∞–ª–≥–æ—Ä–∏—Ç–º –±—ã—Å—Ç—Ä—ã–π, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ Œ± *([—á—Ç–æ —Ç–∞–∫–æ–µ Œ± ?](https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf))*\n",
    "\n",
    "–ó–∞—Ç–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–µ—Ä–∏–º Precision, Recall –∏ Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_alpha = np.arange(1/100000, 20, 0.11) \n",
    "#–≤–æ–∑—å–º–µ–º –ª–∏—Å—Ç —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –æ—Ç 1/100000 –¥–æ 20(–º–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –¥—Ä—É–≥–∏–µ —á–∏—Å–ª–∞,—ç—Ç–∏ –±—ã–ª–∏ –≤—ã–±—Ä–∞–Ω—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ)\n",
    "\n",
    "\n",
    "score_train = np.zeros(len(list_alpha)) # –∑–∞–¥–∞–ª–∏ –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –ø–æ –¥–ª–∏–Ω–µ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –ª–∏—Å—Ç–∞\n",
    "score_test = np.zeros(len(list_alpha))\n",
    "recall_test = np.zeros(len(list_alpha))\n",
    "precision_test= np.zeros(len(list_alpha))\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in list_alpha: # –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Å–ø–∏—Å–∫–µ –∞–ª—å—Ñ\n",
    "    bayes = naive_bayes.MultinomialNB(alpha=alpha) # –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å\n",
    "    bayes.fit(X_train, y_train) # —Ç—Ä–µ–Ω–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å\n",
    "    \n",
    "    score_train[count] = bayes.score(X_train, y_train) # –∏—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –∏–Ω–¥–µ–∫—Å—É –≤ –ª–∏—Å—Ç–µ, –¥–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è accuracy\n",
    "    # score_train? \n",
    "    \n",
    "    score_test[count]= bayes.score(X_test, y_test)\n",
    "    # score_test ? \n",
    "    \n",
    "    recall_test[count] = metrics.recall_score(y_test, bayes.predict(X_test))\n",
    "    precision_test[count] = metrics.precision_score(y_test, bayes.predict(X_test))\n",
    "    \n",
    "    count = count + 1 # –ø–æ—Å–ª–µ –æ–¥–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏, –¥–µ–ª–∞–µ–º –Ω–æ–≤—ã–π —à–∞–≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ 10 –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.matrix(np.c_[list_alpha, score_train, score_test, recall_test, precision_test])\n",
    "models = pd.DataFrame(data = matrix, columns = \n",
    "             ['alpha', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\n",
    "\n",
    "\n",
    "display(models.head(10))\n",
    "\n",
    "display(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "–∞ —Ç–µ–ø–µ—Ä—å –¥–∞–≤–∞–π—Ç–µ –æ—Å—Ç–∞–≤–∏–º –º–æ–¥–µ–ª—å —Å –Ω–∞–∏–≤—ã—Å—à–µ–π test precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = models['Test Precision'].idxmax()\n",
    "best_index\n",
    "\n",
    "# display(best_index)\n",
    "\n",
    "display(models.iloc[best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º, –µ—Å—Ç—å –ª–∏ –º–æ–¥–µ–ª–∏ —Å–æ 100% Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[models['Test Recall']>0.9].sort_values(by=\"Test Precision\", ascending = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–≤–∞–π—Ç–µ –∏–∑ –Ω–∏—Ö –≤—ã–±–µ—Ä–µ–º —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞–∏–±–æ–ª—å—à–µ–µ accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = models[models['Test Precision']==1]['Test Accuracy'].idxmax()\n",
    "\n",
    "models.iloc[best_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### —Å–¥–µ–ª–∞–µ–º –ø—Ä–µ–¥–∏–∫—à–Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes = naive_bayes.MultinomialNB(alpha= 5.830010) # –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "bayes.fit(X_train, y_train) # —Ç—Ä–µ–Ω–∏—Ä—É–µ–º –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "bayes.predict(X_test) # –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "\n",
    "# —Å–¥–µ–ª–∞–µ–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ –¥–ª—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –∫–ª–∞—Å—Å–∞ –∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–≥–æ\n",
    "\n",
    "new_df = pd.DataFrame(data = y_test)\n",
    "new_df[\"predicted\"]=bayes.predict(X_test) # –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¥–∞–≤–∞–π—Ç–µ –ø–æ—Å—Ç—Ä–æ–∏–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ ([–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫](https://en.wikipedia.org/wiki/Confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_confusion_test = metrics.confusion_matrix(y_test, bayes.predict(X_test))\n",
    "\n",
    "\n",
    "pd.DataFrame(data = m_confusion_test, columns = ['Predicted ham', 'Predicted spam'],\n",
    "            index = ['Actual ham', 'Actual spam'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–º–µ—Ç–∏–º, —á—Ç–æ —Ç–∞–∫ –∫–∞–∫ –º—ã –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –ª–µ–π–±–ª spam –∫–∞–∫ –∫–ª–∞—Å—Å 1, –Ω–∞—à–∞ —Ç–∞–±–ª–∏—á–∫–∞ \"–ø–µ—Ä–µ–≤–µ—Ä–Ω—É—Ç–∞\" –∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å True Negative (Actual ham x Predicted ham) \n",
    "–°–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ–ª—É—á–∏–ª–∏—Å—å False Positive? –°–∫–æ–ª—å–∫–æ False Negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM, –º–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ø–æ–ø—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å —Ç—É –∂–µ –∑–∞–¥–∞—á—É, —Å –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_C = np.arange(500, 2000, 100) #100000\n",
    "\n",
    "score_train = np.zeros(len(list_C))\n",
    "score_test = np.zeros(len(list_C))\n",
    "recall_test = np.zeros(len(list_C))\n",
    "precision_test= np.zeros(len(list_C))\n",
    "count = 0\n",
    "for C in list_C:\n",
    "    svc = svm.SVC(C=C)\n",
    "    svc.fit(X_train, y_train)\n",
    "    score_train[count] = svc.score(X_train, y_train)\n",
    "    score_test[count]= svc.score(X_test, y_test)\n",
    "    recall_test[count] = metrics.recall_score(y_test, svc.predict(X_test))\n",
    "    precision_test[count] = metrics.precision_score(y_test, svc.predict(X_test))\n",
    "    count = count + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ 10 –ø–µ—Ä–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∏—Ö –º–µ—Ç—Ä–∏–∫–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.matrix(np.c_[list_C, score_train, score_test, recall_test, precision_test])\n",
    "models = pd.DataFrame(data = matrix, columns = \n",
    "             ['C', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\n",
    "models.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≤—ã–±–µ—Ä–µ–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º test precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = models['Test Precision'].idxmax()\n",
    "\n",
    "display(models.iloc[best_index, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –º–æ–¥–µ–ª–∏ —Å –ø–æ—á—Ç–∏ 100% Precision\n",
    "\n",
    "models[models['Test Precision']>0.99].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = models[models['Test Precision']>0.99]['Test Accuracy'].idxmax()\n",
    "svc = svm.SVC(C=list_C[best_index])\n",
    "svc.fit(X_train, y_train)\n",
    "models.iloc[best_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ø–æ—Å—Ç—Ä–æ–∏–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ –¥–ª—è SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_confusion_test = metrics.confusion_matrix(y_test, svc.predict(X_test))\n",
    "pd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n",
    "            index = ['Actual 0', 'Actual 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
