{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vector semantics-1 (w2v, fasttext)",
      "provenance": [],
      "collapsed_sections": [
        "Ihkv6IG9ZfqC",
        "mvo7AB3h6nAL"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jR05b6WDS17"
      },
      "source": [
        "# Word2vec: работаем с векторными моделями в Python\n",
        "\n",
        "*Эта тетрадка — переосмысленная, дополненная и местами упрощенная мной версия туториала по вордтувеку от Лизы Кузьменко, которая со-основала RusVectores вместе с Андреем Кутузовым*\n",
        "\n",
        "\n",
        "**Word2vec** - библиотека для получения векторных представлений слов на основе их совместной встречаемости в текстах. Вы можете освежить в памяти механизмы работы **word2vec**, прочитав [эту статью](https://vk.com/@sysblok-word2vec-pokazhi-mne-svoi-kontekst-i-ya-skazhu-kto-ty). \n",
        "\n",
        "Сейчас мы научимся использовать **word2vec** в своей повседневной работе. Мы будем использовать реализацию **word2vec** в библиотеке [gensim](https://radimrehurek.com/gensim/) для языка программирования **python**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y4gMOGKqScy"
      },
      "source": [
        "Для работы с эмбеддингами слов существуют и другие библиотеки: кроме [gensim](https://radimrehurek.com/gensim/) можно делать векторные модели в [keras](https://keras.io/), [tensorflow](https://www.tensorflow.org/), [pytorch](https://pytorch.org/). Но мы будем работать с *gensim*, потому что тут это проще и потому что создатели библиотеки моделей RusVectores затачивались под нее.\n",
        "\n",
        "\n",
        "***Gensim***  - изначально библиотека для тематического моделирования текстов. Однако помимо различных алгоритмов для *topic modeling* в ней реализованы на **python** и алгоритмы из тулкита **word2vec** (который в оригинале был написан на C++). Если вы работаете на своей машине и **gensim** у вас не установлен, нужно его установить: `pip3 install gensim`\n",
        "\n",
        "В колабе генсим установлен по умолчанию\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUCfUMi9KDwu"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKROIDiIKEAj"
      },
      "source": [
        "Тьюториал состоит из двух частей:\n",
        "* В первой части мы разберёмся, как загружать уже готовые векторные модели и работать с ними. Например, мы научимся выполнять простые операции над векторами слов, такие как «найти слово с наиболее близким вектором» или «вычислить коэффициент близости между двумя векторами слов». Также мы рассмотрим более сложные операции над векторами, например, «вычесть из вектора слова вектор другого слова», «прибавить к вектору слова вектор другого слова»  «найти лишний вектор в группе слов».\n",
        "\n",
        "* Во второй части мы научимся предобрабатывать текстовые файлы и самостоятельно тренировать векторную модель на своих данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCnjv9MbDS2N"
      },
      "source": [
        "## 1. Работа с готовыми векторными моделями при помощи библиотеки Gensim\n",
        "\n",
        "Для своих индивидуальных нужд и экспериментов бывает полезно самому натренировать модель на нужных данных и с нужными параметрами (об этом раздел 2). Но для каких-то общих целей уже есть готовые модели, в т.ч. для русского языка, обученные на больших корпусах\n",
        "\n",
        "Модели для русского скачать можно здесь - https://rusvectores.org/ru/models/\n",
        "\n",
        "Существуют несколько форматов, в которых могут храниться модели. Во-первых, данные могут храниться в нативном формате *word2vec*, при этом модель может быть бинарной или не бинарной. Для загрузки модели в формате *word2vec* в классе `KeyedVectors` (в котором хранится большинство относящихся к дистрибутивным моделям функций) существует функция `load_word2vec_format`, а бинарность модели можно указать в аргументе `binary` (внизу будет пример). Помимо этого, модель можно хранить и в собственном формате *gensim*, для этого существует класс `Word2Vec` с функцией `load`. Поскольку модели бывают разных форматов, то для них написаны разные функции загрузки; бывает полезно учитывать это в своем скрипте. Наш код определяет тип модели по её расширению, но вообще файл с моделью может называться как угодно, жестких ограничений для расширения нет.\n",
        "\n",
        "Давайте скачаем новейшую модель для русского языка, созданную на основе [Национального корпуса русского языка (НКРЯ)](http://www.ruscorpora.ru/) (поскольку zip-архив с моделью весит почти 500 мегабайт, следующая ячейка выполнится у вас не сразу!). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ4LBAljIr23"
      },
      "source": [
        "!wget 'http://vectors.nlpl.eu/repository/20/180.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90maopMCQAow"
      },
      "source": [
        "Теперь моделька в виде zip-архива лежит у нас в рабочей папке"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uSZXD8NI9fX"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp6UO3tqJQhh"
      },
      "source": [
        "Распаковывать скачанный архив для обычных моделей не нужно, так как его содержимое прочитается при помощи специальной инструкции:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79IZGnFGMWiz"
      },
      "source": [
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvYOxVRADS2O"
      },
      "source": [
        "with zipfile.ZipFile('180.zip', 'r') as archive:\n",
        "    stream = archive.open('model.bin')\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH5xB1ecK8KB"
      },
      "source": [
        "### Все, этой моделью уже можно пользоваться для оценки семантической близости:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk6fX0O1New_"
      },
      "source": [
        "Выводим 10 соседей слова по близости и меру близости с ними — метод `most_similar`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHn2dXOITmjI"
      },
      "source": [
        "model.most_similar ('кофе_NOUN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTW4Tiv67Lig"
      },
      "source": [
        "⛳ 💻  Вопрос: как получить больше 10?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6KfUGn27uze"
      },
      "source": [
        "## ваш код\n",
        "model.most_similar ('лингвист_NOUN', topn=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFrok1K1NWQb"
      },
      "source": [
        "model.most_similar ('программирование_NOUN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by3TyepsTo_2"
      },
      "source": [
        "model.most_similar ('программировать_VERB', topn=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA1eGZ4jT6vh"
      },
      "source": [
        "### 🤔 стоп, а что за _NOUN, _VERB это мне всегда руками так писать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyBvufWTSzPQ"
      },
      "source": [
        "Cлова в модель надо подавать с указанием части речи (pos tag) из набора тегов [Universal POS-tags](https://universaldependencies.org/u/pos/). Так устроены модели RusVectores — в них снята омонимия на уровне словоформ:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um5xgucsSyVS"
      },
      "source": [
        "model.most_similar ('печь_NOUN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yra9RrYS7gn"
      },
      "source": [
        "model.most_similar ('печь_VERB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH1mTtXeW9Mw"
      },
      "source": [
        "Давайте пока работаем с небольшим числом слов писать это руками. А дальше поговорим, как с этим работать автоматически, если анализируем большой текст. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kVL3_lSDS2P"
      },
      "source": [
        "Если мы прогоняем много слов, стоит вставить проверку, что слова нет в модели. Допустим, нам интересны такие слова (пример для русского языка):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw-DIYJwDS2P"
      },
      "source": [
        "words = ['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'студент_NOUN', 'студент_ADJ']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGgjvAo7DS2P"
      },
      "source": [
        "Попросим у модели 10 ближайших соседей для каждого слова и коэффициент косинусной близости для каждого:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftf_MlfKDS2P"
      },
      "source": [
        "for word in words:\n",
        "    # есть ли слово в модели? Может быть, и нет\n",
        "    if word in model:\n",
        "        print(word)\n",
        "        # выдаем 10 ближайших соседей слова:\n",
        "        for i in model.most_similar(positive=[word], topn=10):\n",
        "            # слово + коэффициент косинусной близости\n",
        "            print(i[0], i[1])\n",
        "        print('\\n')\n",
        "    else:\n",
        "        # Увы!\n",
        "        print(word + ' is not present in the model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5FVSQtxU51J"
      },
      "source": [
        "Наш код сказал нам, что прилагательного студент не модель не знает..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7YE0Pf8XLZ6"
      },
      "source": [
        "### сравнить близость 2 слов:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbGlUQJyDS2Q"
      },
      "source": [
        "Находим косинусную близость пары векторов слов — метод `similarity`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8A_SmuvDS2Q"
      },
      "source": [
        "print(model.similarity('кофе_NOUN', 'чай_NOUN'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m5bK1E8LPOD"
      },
      "source": [
        "print(model.similarity('компот_NOUN', 'чай_NOUN'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIUfy1H3LD04"
      },
      "source": [
        "print(model.similarity('чай_NOUN', 'картофель_NOUN'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmNJWJYwQcXZ"
      },
      "source": [
        "<img src=\"https://www.meme-arsenal.com/memes/32687f97c291d55dc3143e26821ff4d4.jpg\">\n",
        "\n",
        "Мистер картошка, вы раскрыты! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doqTxplCDS2Q"
      },
      "source": [
        "## 2. Более сложные операции над векторами"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sXaupy_DS2R"
      },
      "source": [
        "Помимо более простых операций над векторами (нахождение косинусной близости между двумя векторами и ближайших соседей вектора) **gensim** позволяет выполнять и более сложные операции над несколькими векторами. Так, например, мы можем найти лишнее слово в группе. Лишним словом является то, вектор которого наиболее удален от других векторов слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vQRNcaJN-z0"
      },
      "source": [
        "words = ['яблоко_NOUN',\n",
        " 'груша_NOUN',\n",
        " 'виноград_NOUN',\n",
        " 'банан_NOUN',\n",
        " 'лимон_NOUN',\n",
        " 'картофель_NOUN', 'лошадь_NOUN', 'философия_NOUN', 'видеоблог_NOUN']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByxHRCybDS2R"
      },
      "source": [
        "print(model.doesnt_match(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4vFdIVUDS2R"
      },
      "source": [
        "Также можно складывать и вычитать вектора нескольких слов. Например, сложив два вектора и вычтя из них третий вектор, мы можем решить своеобразную пропорцию. Подробнее о семантических пропорциях вы можете прочитать в [материале Системного Блока](https://vk.com/@sysblok-vo-chto-prevraschaetsya-zhizn-bez-lubvi)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKymEqOIO912"
      },
      "source": [
        "Меня всегда радует, что вот это реально работает: \n",
        "🦅 -> 🐠"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD0rpVnpDS2R"
      },
      "source": [
        "print(model.most_similar(positive=['машина_NOUN', 'крыло_NOUN'], negative=['колесо_NOUN'])[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drYgxafWPY3N"
      },
      "source": [
        "В обратную сторону тоже: 🐠 -> 🦅 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDeoPloNO4U-"
      },
      "source": [
        "print(model.most_similar(positive=['птица_NOUN', 'плавник_NOUN'], negative=['крыло_NOUN'])[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQkg4o54D4Lr"
      },
      "source": [
        "Ну и конечно: 💔"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLuIY3z3Xqjg"
      },
      "source": [
        "print(model.most_similar(positive=['жизнь_NOUN'], negative=['любовь_NOUN'])[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihkv6IG9ZfqC"
      },
      "source": [
        "### ⛳ 💻  Задание: подберите еще 3-4 симпатичных примера векторной арифметики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYHaaRtdZRqn"
      },
      "source": [
        "## 3. Предобработка текстовых данных\n",
        "\n",
        "Вернемся к вопросу о pos-тегах (_NOUN, _VERB и проч). Пока мы обходились без них, но это выглядело как костыль, правда же 🔩 Если мы хотим обрабатывать свои тексты — нам надо бы научиться предобрабатывать их так, чтобы каждое слово шло именно с таким тегом. Тогда можно будет гонять на них модели word2vec от RusVectores (а это лучшее что есть для русского и вообще такой стандартный стандарт).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHyLP1pJknEe"
      },
      "source": [
        "Предобработка текстов для тренировки моделей выглядит следующим образом:\n",
        "* сначала мы приведем все слова к начальной форме (лемматизируем) и удалим стоп-слова;\n",
        "* затем мы приведем все леммы к нижнему регистру;\n",
        "* для каждого слова добавим его частеречный тэг.\n",
        "\n",
        "Давайте попробуем воссоздать процесс предобработки текста на примере [сказки Хармса](https://raw.githubusercontent.com/dhhse/dh2020/master/data/harms.txt). Для предобработки можно использовать различные тэггеры, мы сейчас будем использовать [*UDPipe*](https://ufal.mff.cuni.cz/udpipe), чтобы сразу получить частеречную разметку в виде Universal POS-tags. Сначала установим обертку *UDPipe* для Python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HmnqWoUkmZq"
      },
      "source": [
        "!pip3 install ufal.udpipe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWpExSiWkqNA"
      },
      "source": [
        "*UDPipe* использует предобученные модели для лемматизации и тэггинга. Вы можете использовать [уже готовую модель](https://rusvectores.org/static/models/udpipe_syntagrus.model) или обучить свою. \n",
        "\n",
        "Кусок кода ниже скачает модель UDPipe для лингвистической предобработки. Модель весит 40 мегабайт, поэтому ячейка может выполнятся некоторое время, особенно если у вас небыстрый интернет. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUVSkATuDS2F"
      },
      "source": [
        "!wget 'https://rusvectores.org/static/models/udpipe_syntagrus.model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "KrrsBoIP8OK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DptC4KhnnTF"
      },
      "source": [
        "modelfile = 'udpipe_syntagrus.model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8kt36LnooJa"
      },
      "source": [
        "## ⚠️ Для соответствия моделям RusVectores требуется еще немножко допиливания напильником поверх UDPipe. Да и сама машинерия UDPipe довольно громоздко устроена (там выдача [в формате CONLLU](https://universaldependencies.org/format.html)). Поэтому ниже я просто переиспользую функции, которые написала со-авторка RusVectores Лиза Кузьменко для предобработки при помощи UDPipe и использую их. Но в целом там вроде бы ничего магического не происходит:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e8JL8zNDS2G"
      },
      "source": [
        "Приступим к собственно предобработке текста. Попробуем лемматизировать текст и добавить частеречные тэги при помощи этой функции:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvGudRJeDS2G"
      },
      "source": [
        "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
        "    entities = {'PROPN'}\n",
        "    named = False\n",
        "    memory = []\n",
        "    mem_case = None\n",
        "    mem_number = None\n",
        "    tagged_propn = []\n",
        "\n",
        "    # обрабатываем текст, получаем результат в формате conllu:\n",
        "    processed = pipeline.process(text)\n",
        "\n",
        "    # пропускаем строки со служебной информацией:\n",
        "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
        "\n",
        "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
        "    tagged = [w.split('\\t') for w in content if w]\n",
        "\n",
        "    for t in tagged:\n",
        "        if len(t) != 10:\n",
        "            continue\n",
        "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
        "        if not lemma or not token:\n",
        "            continue\n",
        "        if pos in entities:\n",
        "            if '|' not in feats:\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "                continue\n",
        "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
        "            if 'Case' not in morph or 'Number' not in morph:\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "                continue\n",
        "            if not named:\n",
        "                named = True\n",
        "                mem_case = morph['Case']\n",
        "                mem_number = morph['Number']\n",
        "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
        "                memory.append(lemma)\n",
        "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
        "                    named = False\n",
        "                    past_lemma = '::'.join(memory)\n",
        "                    memory = []\n",
        "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
        "            else:\n",
        "                named = False\n",
        "                past_lemma = '::'.join(memory)\n",
        "                memory = []\n",
        "                tagged_propn.append(past_lemma + '_PROPN ')\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "        else:\n",
        "            if not named:\n",
        "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
        "                    lemma = num_replace(token)\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "            else:\n",
        "                named = False\n",
        "                past_lemma = '::'.join(memory)\n",
        "                memory = []\n",
        "                tagged_propn.append(past_lemma + '_PROPN ')\n",
        "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
        "\n",
        "    if not keep_punct:\n",
        "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
        "    if not keep_pos:\n",
        "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
        "    return tagged_propn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-StllA8WDS2H"
      },
      "source": [
        "Эту функцию можно также изменить под конкретную задачу. Например, если частеречные тэги нам не нужны, в функции ниже выставим `keep_pos=False`. Если необходимо сохранить знаки пунктуации, можно выставить `keep_punct=True`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK6OaUjkDS2H"
      },
      "source": [
        "Теперь загружаем модель *UDPipe*, читаем текстовый файл и обрабатываем его при помощи нашей функции. В файле должен содержаться необработанный текст (одно предложение на строку или один абзац на строку).\n",
        "Этот текст токенизируется, лемматизируется и размечается по частям речи с использованием UDPipe.\n",
        "На выход мы получаем последовательность разделенных пробелами лемм с частями речи (\"зеленый\\_NOUN трамвай\\_NOUN\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYFeA8-nmkIY"
      },
      "source": [
        "# в функции ниже используется питоновский модуль wget (не то же самое, что !wget выше)\n",
        "# на тот случай, если модель не скачана -- он ее автоматически перескачает\n",
        "# поэтому в этой ячейке установим питоновский wget\n",
        "!pip3 install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_G3ibIcDS2I"
      },
      "source": [
        "from ufal.udpipe import Model, Pipeline\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import wget\n",
        "\n",
        "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
        "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
        "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
        "\n",
        "    if not os.path.isfile(modelfile):\n",
        "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
        "        wget.download(udpipe_model_url)\n",
        "\n",
        "    print('\\nLoading the model...', file=sys.stderr)\n",
        "    model = Model.load(modelfile)\n",
        "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
        "\n",
        "    print('Processing input...', file=sys.stderr)\n",
        "    lines = text.split('\\n')\n",
        "    tagged = []\n",
        "    for line in lines:\n",
        "        # line = unify_sym(line.strip()) # здесь могла бы быть ваша функция очистки текста\n",
        "        output = process(process_pipeline, text=line)\n",
        "        tagged_line = ' '.join(output)\n",
        "        tagged.append(tagged_line)\n",
        "    return '\\n'.join(tagged)\n",
        "\n",
        "def num_replace(word):\n",
        "    newtoken = 'x' * len(word)\n",
        "    return newtoken"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VcD_86bAl0t"
      },
      "source": [
        "Скачаем теперь текст, с которым будем работать:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDqsRQ5JmFad"
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/dhhse/dh2020/master/data/harms.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Ys8_xQDS2J"
      },
      "source": [
        "text = open('harms.txt', 'r', encoding='utf-8').read()\n",
        "\n",
        "processed_text = tag_ud(text=text, modelfile=modelfile)\n",
        "\n",
        "print(processed_text[:350])\n",
        "\n",
        "with open('harms_processed.txt', 'w', encoding='utf-8') as out:\n",
        "    out.write(processed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L__c7FzUDS2K"
      },
      "source": [
        "Наша функция напечатает обработанный текст, который мы теперь можем также сохранить в файл. \n",
        "\n",
        "Итак, в ходе этой части тьюториала мы научились от \"сырого текста\" приходить к лемматизированному тексту с частеречными тэгами, который уже можно подавать на вход модели! \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erNk9KEo9R2E"
      },
      "source": [
        "with open('harms_processed.txt', 'r', encoding='utf-8') as tagged_text:\n",
        "    words = tagged_text.read().split()\n",
        "print(len(words))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " for word in words[140:150]:\n",
        "      print ('слово: ', word)\n",
        "      if word in model:\n",
        "        x = model.most_similar(word)\n",
        "        print ('ближайший синоним: ', x[0][0],\"\\n\")"
      ],
      "metadata": {
        "id": "pmTdQo6NFCzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjx6ZCuxAzFG"
      },
      "source": [
        "[Вернемся к слайдам ненадолго](https://docs.google.com/presentation/d/11fYkNG1IFBJzVQ27LNxaE_fj8XHO40JxYaUUSzpcCKQ/edit#slide=id.gd5da728dca_0_1009) — нам осталась fun part сегодняшней пары! 🎪"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTkDErYXA2t1"
      },
      "source": [
        "⛳ 💻  Я предлагаю вам реализовать свою версию  [векторных романов Б. Орехова](https://habr.com/ru/post/326380/). В базовом варианте предлагаю делать упрощенно: без восстановления морфологической формы после замены слова на его векторный синоним (можем для корректности назвать это семантическим ассоциатом или квази-синонимом). Ну то есть заменяем \"бегемотом\" на \"гиппопотам\" (или что там выдаст word2vec), но форму \"гиппопотамом\" уже не восстанавливаем. \n",
        "\n",
        "А кто чувствует в себе силы — делайте полную версию, с восстановлением исходной грамматической формы \"гиппопотамом\". За это оценка будет выше. Ну и результат у вас будет гораздо прикольнее, потому что текст будет довольно читабелен.\n",
        "\n",
        "К <b><s>10</s> 16 мая</b> нужно сдать любое крупное произведение русской литературы, в котором слова заменены на их векторные семантические ассоциаты. Ну и, конечно, код, который это делает. Код из этой тетрадки можно переиспользовать."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqzH2MJbp-3t"
      },
      "source": [
        "## 4. Тренируем свою модель в gensim (остается на домашние эксперименты и следующий раз)\n",
        "\n",
        "Поскольку обучение и загрузка моделей могут занимать продолжительное время, иногда бывает полезно вести лог событий. Для этого используется стандартная питоновская библиотека **logging**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6P23Mg6DS2L"
      },
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRdlTAkXDS2L"
      },
      "source": [
        "На вход модели мы даем наш обработанный текстовый файл (либо любой другой текст, важно лишь, что каждое предложение должно быть на отдельной строчке)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aQoK8ydDS2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90f013b-43b3-411b-b926-baaa78db07bd"
      },
      "source": [
        "f = 'harms_processed.txt'\n",
        "data = gensim.models.word2vec.LineSentence(f)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.LineSentence at 0x7f0d3719cdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkxT-JJqDS2L"
      },
      "source": [
        "Инициализируем модель. Параметры в скобочках:\n",
        "* data - данные, \n",
        "* size - размер вектора, \n",
        "* window - размер окна наблюдения,\n",
        "* min_count - мин. частотность слова в корпусе, которое мы берем,\n",
        "* sg - используемый алгоритм обучение (0 - CBOW, 1 - Skip-gram))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDPJ3kEPjQLZ"
      },
      "source": [
        "model = gensim.models.Word2Vec( data, size=500, window=10, min_count=2, sg=0, iter=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wU8u-WbDS2M"
      },
      "source": [
        "# вариант для новой версии gensim:\n",
        "# model = gensim.models.Word2Vec(data, vector_size=500, window=10, min_count=2, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNWoIa4wDS2M"
      },
      "source": [
        "Мы создаем модель, в которой размерность векторов — 500, размер окна наблюдения — 10 слов, алгоритм обучения — CBOW, слова, встретившиеся в корпусе только 1 раз, не используются. После тренировки модели можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImrKLGf9DS2M"
      },
      "source": [
        "model.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_bxgIYeDS2M"
      },
      "source": [
        "Смотрим, сколько в модели слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YOCjNmWDS2N"
      },
      "source": [
        "print(len(model.wv.vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km8YogT4DS2N"
      },
      "source": [
        "И сохраняем!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrfSGHKhDS2N"
      },
      "source": [
        "model.save('my.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS5ben4LwMth"
      },
      "source": [
        "model.most_similar ('королева_NOUN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbVMzxuyl9TC"
      },
      "source": [
        "# для новой версии генсим, где не работает просто most_similar:\n",
        "model.wv.most_similar ('карандаш_NOUN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F2mDTjibNB57"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvo7AB3h6nAL"
      },
      "source": [
        "### ⛳ 💻 Задание\n",
        "\n",
        "Задание: попробуйте обучить свою модель на каких-нибудь текстах. Например, [вот текст \"Войны и мира\"](https://github.com/dhhse/dh2020/blob/master/data/wap_w2v.txt), в котором каждое предложение с новой строки. Обучите модель на нем. Исследуйте близости слов. \n",
        "\n",
        "*   Подсказка: вам точно понадобится `gensim.models.word2vec.LineSentence` (см. выше) чтобы преобразовать текст в формат, который можно передавать для обучения модели\n",
        "*   Подсказка 2: а еще вам понадобится `gensim.models.Word2Vec` (см. выше)\n",
        "*   Подсказка 3: без остального в принципе можно обойтись. Но если подать тексты сырыми, то модель обучится на токенах. \"Кот\", \"кот\" и \"кота\" будут для нее разными словами. Зато \"печь\" будет одним словом вне завимисимости от части речи. Поэтому можно еще воспользоваться функцией `tag_ud` выше и обучить модель как у русвекторес. Но осторожно. UDPipe-ом обработать всю Войну и Мир — это минут 6-7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYX3PTisK143"
      },
      "source": [
        "Без предобработки ваша модель сможет как-то вот так: \n",
        "\n",
        "<img src = \"https://github.com/dhhse/dh2020/raw/master/pics/napoleon_token.png\">\n",
        "<img src = \"https://github.com/dhhse/dh2020/raw/master/pics/war_token.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjLGKy6JMPZG"
      },
      "source": [
        "C предобработкой будет как в моделях русвекторес: лемма с пос-тегом. И это на русском работает осмысленнее обычно: \n",
        "\n",
        "<img src = \"https://github.com/dhhse/dh2020/raw/master/pics/war_lemma_pos.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueUidTPFbkqI",
        "outputId": "adcf00ad-6f99-48c6-fea1-5069230bbb21"
      },
      "source": [
        "!wget 'https://github.com/dhhse/dh2020/raw/master/data/wap_w2v.txt' "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-13 15:53:10--  https://github.com/dhhse/dh2020/raw/master/data/wap_w2v.txt\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dhhse/dh2020/master/data/wap_w2v.txt [following]\n",
            "--2021-05-13 15:53:10--  https://raw.githubusercontent.com/dhhse/dh2020/master/data/wap_w2v.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5171830 (4.9M) [text/plain]\n",
            "Saving to: ‘wap_w2v.txt’\n",
            "\n",
            "wap_w2v.txt         100%[===================>]   4.93M  25.6MB/s    in 0.2s    \n",
            "\n",
            "2021-05-13 15:53:11 (25.6 MB/s) - ‘wap_w2v.txt’ saved [5171830/5171830]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK9p9ez4br0m"
      },
      "source": [
        "wap = 'wap_w2v.txt'\n",
        "data = gensim.models.word2vec.LineSentence(wap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_TLGKKVb92E",
        "outputId": "486fa883-236c-4035-ce6e-d7990cac9649"
      },
      "source": [
        "model_wap = gensim.models.Word2Vec(data, size=500, window=10, min_count=2, sg=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-13 15:54:45,536 : INFO : collecting all words and their counts\n",
            "2021-05-13 15:54:45,539 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-05-13 15:54:45,628 : INFO : PROGRESS: at sentence #10000, processed 161603 words, keeping 39106 word types\n",
            "2021-05-13 15:54:45,717 : INFO : PROGRESS: at sentence #20000, processed 335781 words, keeping 65326 word types\n",
            "2021-05-13 15:54:45,786 : INFO : collected 81592 word types from a corpus of 462159 raw words and 27123 sentences\n",
            "2021-05-13 15:54:45,789 : INFO : Loading a fresh vocabulary\n",
            "2021-05-13 15:54:45,864 : INFO : effective_min_count=2 retains 30236 unique words (37% of original 81592, drops 51356)\n",
            "2021-05-13 15:54:45,866 : INFO : effective_min_count=2 leaves 410803 word corpus (88% of original 462159, drops 51356)\n",
            "2021-05-13 15:54:45,959 : INFO : deleting the raw counts dictionary of 81592 items\n",
            "2021-05-13 15:54:45,964 : INFO : sample=0.001 downsamples 36 most-common words\n",
            "2021-05-13 15:54:45,966 : INFO : downsampling leaves estimated 342650 word corpus (83.4% of prior 410803)\n",
            "2021-05-13 15:54:46,043 : INFO : estimated required memory for 30236 words and 500 dimensions: 136062000 bytes\n",
            "2021-05-13 15:54:46,044 : INFO : resetting layer weights\n",
            "2021-05-13 15:54:52,191 : INFO : training model with 3 workers on 30236 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2021-05-13 15:54:53,245 : INFO : EPOCH 1 - PROGRESS: at 66.05% examples, 211092 words/s, in_qsize 5, out_qsize 0\n",
            "2021-05-13 15:54:53,673 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 15:54:53,688 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 15:54:53,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 15:54:53,707 : INFO : EPOCH - 1 : training on 462159 raw words (342450 effective words) took 1.5s, 226671 effective words/s\n",
            "2021-05-13 15:54:54,802 : INFO : EPOCH 2 - PROGRESS: at 67.85% examples, 211605 words/s, in_qsize 5, out_qsize 1\n",
            "2021-05-13 15:54:55,172 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 15:54:55,194 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 15:54:55,221 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 15:54:55,222 : INFO : EPOCH - 2 : training on 462159 raw words (342366 effective words) took 1.5s, 226933 effective words/s\n",
            "2021-05-13 15:54:56,248 : INFO : EPOCH 3 - PROGRESS: at 57.21% examples, 188463 words/s, in_qsize 5, out_qsize 0\n",
            "2021-05-13 15:54:56,830 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 15:54:56,859 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 15:54:56,869 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 15:54:56,870 : INFO : EPOCH - 3 : training on 462159 raw words (342493 effective words) took 1.6s, 208929 effective words/s\n",
            "2021-05-13 15:54:57,884 : INFO : EPOCH 4 - PROGRESS: at 66.05% examples, 219679 words/s, in_qsize 5, out_qsize 0\n",
            "2021-05-13 15:54:58,354 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 15:54:58,359 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 15:54:58,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 15:54:58,372 : INFO : EPOCH - 4 : training on 462159 raw words (342766 effective words) took 1.5s, 229066 effective words/s\n",
            "2021-05-13 15:54:59,413 : INFO : EPOCH 5 - PROGRESS: at 63.81% examples, 207187 words/s, in_qsize 5, out_qsize 0\n",
            "2021-05-13 15:54:59,874 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 15:54:59,888 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 15:54:59,909 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 15:54:59,910 : INFO : EPOCH - 5 : training on 462159 raw words (342580 effective words) took 1.5s, 223881 effective words/s\n",
            "2021-05-13 15:54:59,914 : INFO : training on a 2310795 raw words (1712655 effective words) took 7.7s, 221801 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7siNtzVcDNR",
        "outputId": "3994f30f-6701-44b2-b1b1-fb36751aadec"
      },
      "source": [
        "model_wap.most_similar('Наташа')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('долго', 0.9984106421470642),\n",
              " ('Соню,', 0.9982855319976807),\n",
              " ('что-то', 0.9982360601425171),\n",
              " ('нее,', 0.9981175065040588),\n",
              " ('за', 0.9978007078170776),\n",
              " ('Наташе,', 0.9977572560310364),\n",
              " ('грудь', 0.997657299041748),\n",
              " ('заметила,', 0.9976304769515991),\n",
              " ('слушал', 0.9974093437194824),\n",
              " ('где', 0.9973325729370117)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np3N5_1EgpiM",
        "outputId": "376e94f8-9717-4738-98f5-6e5c1bfdb558"
      },
      "source": [
        "len(model_wap.wv.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT2ehmkHcbhq",
        "outputId": "023d175e-a964-4936-ce5c-e0a533dd5eb1"
      },
      "source": [
        "text = open('wap_w2v.txt', 'r', encoding='utf-8').read()\n",
        "processed_text = tag_ud(text=text, modelfile=modelfile)\n",
        "print(processed_text[:350])\n",
        "with open('wap_w2v_processed.txt', 'w', encoding='utf-8') as out:\n",
        "    out.write(processed_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading the model...\n",
            "Processing input...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "то_PRON первый_ADJ\n",
            "\n",
            "часть_NOUN первый_ADJ\n",
            "\n",
            "i_NUM\n",
            "\n",
            "Eh_PROPN bien_X mon_X prince_X\n",
            "Gênes_PROPN et_X Lucques_PROPN ne_X sont_X plus_X que_X des_X apanages_X des_X поместье_NOUN de_X la_X famille_X Buonaparte_PROPN\n",
            "Non_PROPN je_X vous_X préviens_X que_X si_X vous_X ne_X me_X dites_X pas_X que_X nous_X avons_X la_X guerre_X si_X vous_X vous_X permettez_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3f-1IrFctHA"
      },
      "source": [
        "data = gensim.models.word2vec.LineSentence('wap_w2v_processed.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr_AKEk-c0kb",
        "outputId": "e69f4bef-3dac-402c-d1a7-1a8725545b94"
      },
      "source": [
        "model_wap_lemmas_pos = gensim.models.Word2Vec(data, size=300, window=10, min_count=2, sg=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-13 16:07:24,984 : INFO : collecting all words and their counts\n",
            "2021-05-13 16:07:24,987 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2021-05-13 16:07:25,076 : INFO : PROGRESS: at sentence #10000, processed 156252 words, keeping 15798 word types\n",
            "2021-05-13 16:07:25,181 : INFO : PROGRESS: at sentence #20000, processed 324927 words, keeping 24155 word types\n",
            "2021-05-13 16:07:25,263 : INFO : collected 29037 word types from a corpus of 447977 raw words and 27064 sentences\n",
            "2021-05-13 16:07:25,269 : INFO : Loading a fresh vocabulary\n",
            "2021-05-13 16:07:25,300 : INFO : effective_min_count=2 retains 13506 unique words (46% of original 29037, drops 15531)\n",
            "2021-05-13 16:07:25,301 : INFO : effective_min_count=2 leaves 432446 word corpus (96% of original 447977, drops 15531)\n",
            "2021-05-13 16:07:25,343 : INFO : deleting the raw counts dictionary of 29037 items\n",
            "2021-05-13 16:07:25,346 : INFO : sample=0.001 downsamples 50 most-common words\n",
            "2021-05-13 16:07:25,348 : INFO : downsampling leaves estimated 341275 word corpus (78.9% of prior 432446)\n",
            "2021-05-13 16:07:25,381 : INFO : estimated required memory for 13506 words and 300 dimensions: 39167400 bytes\n",
            "2021-05-13 16:07:25,382 : INFO : resetting layer weights\n",
            "2021-05-13 16:07:28,074 : INFO : training model with 3 workers on 13506 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2021-05-13 16:07:29,084 : INFO : EPOCH 1 - PROGRESS: at 90.44% examples, 302352 words/s, in_qsize 5, out_qsize 0\n",
            "2021-05-13 16:07:29,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 16:07:29,152 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 16:07:29,156 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 16:07:29,158 : INFO : EPOCH - 1 : training on 447977 raw words (341385 effective words) took 1.1s, 316128 effective words/s\n",
            "2021-05-13 16:07:30,173 : INFO : EPOCH 2 - PROGRESS: at 92.46% examples, 309685 words/s, in_qsize 4, out_qsize 0\n",
            "2021-05-13 16:07:30,197 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 16:07:30,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 16:07:30,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 16:07:30,230 : INFO : EPOCH - 2 : training on 447977 raw words (341314 effective words) took 1.1s, 320855 effective words/s\n",
            "2021-05-13 16:07:31,245 : INFO : EPOCH 3 - PROGRESS: at 92.47% examples, 309533 words/s, in_qsize 4, out_qsize 0\n",
            "2021-05-13 16:07:31,289 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 16:07:31,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 16:07:31,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 16:07:31,317 : INFO : EPOCH - 3 : training on 447977 raw words (341496 effective words) took 1.1s, 316306 effective words/s\n",
            "2021-05-13 16:07:32,327 : INFO : EPOCH 4 - PROGRESS: at 83.56% examples, 280491 words/s, in_qsize 5, out_qsize 0\n",
            "2021-05-13 16:07:32,449 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 16:07:32,455 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 16:07:32,477 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 16:07:32,478 : INFO : EPOCH - 4 : training on 447977 raw words (341282 effective words) took 1.2s, 295714 effective words/s\n",
            "2021-05-13 16:07:33,499 : INFO : EPOCH 5 - PROGRESS: at 94.72% examples, 314847 words/s, in_qsize 3, out_qsize 0\n",
            "2021-05-13 16:07:33,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2021-05-13 16:07:33,535 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2021-05-13 16:07:33,540 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2021-05-13 16:07:33,541 : INFO : EPOCH - 5 : training on 447977 raw words (341338 effective words) took 1.1s, 323064 effective words/s\n",
            "2021-05-13 16:07:33,543 : INFO : training on a 2239885 raw words (1706815 effective words) took 5.5s, 312117 effective words/s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXvtzGvbc2Gu",
        "outputId": "eff19262-e9d1-4d33-ed4d-cc4bbbc48154"
      },
      "source": [
        "model_wap_lemmas_pos.most_similar ('Наташа_PROPN')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Пьер_PROPN', 0.9753262996673584),\n",
              " ('Марья_PROPN', 0.9669135808944702),\n",
              " ('осведомляться_VERB', 0.9598015546798706),\n",
              " ('удивление_NOUN', 0.9585548639297485),\n",
              " ('письмо_NOUN', 0.9561311602592468),\n",
              " ('отъезд_NOUN', 0.9557530283927917),\n",
              " ('знакомый_NOUN', 0.9538062810897827),\n",
              " ('холодность_NOUN', 0.9526064395904541),\n",
              " ('разговор_NOUN', 0.9524383544921875),\n",
              " ('смутный_ADJ', 0.9505127668380737)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bm2exnhMfhAQ",
        "outputId": "c91145d0-e6d1-437a-be51-ef5e826266f4"
      },
      "source": [
        "len(model_wap_lemmas_pos.wv.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13506"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KouFnFeLM7a"
      },
      "source": [
        "\n",
        "## FastText: эмбеддинги n-граммов\n",
        "\n",
        "FastText использует не только эмбеддинги слов, но и эмбеддинги n-грамов. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2ssCoFXUeOH"
      },
      "source": [
        "### Для работы с fasttext-моделью придется обновить gensim\n",
        "\n",
        "Корректная работа с fasttext-моделями гарантируется от версии 3.7.2, а в колабе по умолчанию генсим 3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5eXNJ1qqOeh"
      },
      "source": [
        "import  gensim\n",
        "gensim.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf3zrCMDRdlQ"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgmcSYO0UsY9"
      },
      "source": [
        "После этого надо перезапустить среду (колаб и сам вам предложит это сделать)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9SBS53pRx9i"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH5lqIVqRS7z"
      },
      "source": [
        "gensim.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zsJhprjVKxB"
      },
      "source": [
        "### Отлично, теперь возьмем русскую фасттекст-модель \n",
        "из уже известной нам [коллекции](https://rusvectores.org/ru/models/) RusVectores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubhi2tebLLuc"
      },
      "source": [
        "!wget 'http://vectors.nlpl.eu/repository/20/214.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeHEbke4pY3Y"
      },
      "source": [
        "В генсим её надо загружать чуть иначе, чем word2vec-овскую модель. Надо сначала распаковать архив:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Fb2rZl8TJmD"
      },
      "source": [
        "!unzip '214.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf0wT0t5TWer"
      },
      "source": [
        "fasttext_model = gensim.models.KeyedVectors.load('model.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mm1temk59fS"
      },
      "source": [
        "### косинусная близость на примерах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM7-Bw4KXUOU"
      },
      "source": [
        "fasttext_model.most_similar ('кравать') # попробовать разные опечатки"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktjm7Nl9qz_q"
      },
      "source": [
        "fasttext_model.most_similar ('котэ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeGPwlGwUyov"
      },
      "source": [
        "fasttext_model.most_similar ('некузявый')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBkb9rFHTeDa"
      },
      "source": [
        "fasttext_model.most_similar ('лебедиво')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1KtLjSxY_QP"
      },
      "source": [
        "fasttext_model.most_similar ('lol') # латиницу русская модель не делает, делает веселое"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-CbFK5nvwDB"
      },
      "source": [
        "## Как выйти за пределы слов и применить это на практике?\n",
        "\n",
        "Как я уже говорил, векторные модели сделали не для того, чтобы веселиться с векторной математикой (ну или не только для этого). В первую очередь это супер-полезный способ **векторизации текста** для практических задач комплингвистики:\n",
        "\n",
        "\n",
        "*   Классификация текстов\n",
        "*   Извлечение информации (которое часто сводится к задаче классификации слов или их последовательностей)\n",
        "*   Анализ тональности\n",
        "*   И прочее\n",
        "\n",
        "Если объектом является не отдельное слово, а предложение или текст (так бывает часто, например, при решении все той же задачи классификации текстов) то общая идея такая: весь текст превращается в вектор (набор чиселок), которые как-то зависят от векторов его слов (множества наборов чиселок). \n",
        "\n",
        "Самый очевидный вариант — сложить вектора всех слов текста или взять средний вектор всех слов текста.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3ZVk0wtPufH"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMiEuiYpmjA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ecb559-f8a4-49c2-e897-bfaac7976345"
      },
      "source": [
        "fasttext_model.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(347295, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzj1AmjUOhRM"
      },
      "source": [
        "np.average (model.wv['глокая'], model.wv['куздра'], model.wv['штеко'] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z19eQyiZ0od0"
      },
      "source": [
        "Чуть более тонкое есть в алгоритме doc2vec — его сделали те же люди, что и word2vec. Они придумали как бы добавлять еще одно псевдо-слово в контекст при обучении векторов слов для word2vec. В результате у нас после обучения кроме векторов слов есть еще один вектор той же размерности, который как бы побывал в контексте всех слов данного документа. Он и выдается в качестве вектора (эмбеддинга) документа. \n",
        "\n",
        "<img src = \"https://miro.medium.com/max/535/0*x-gtU4UlO8FAsRvL.\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clXPsDp5OGYv"
      },
      "source": [
        "Реализация `doc2vec`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Uyx_w5DS2R"
      },
      "source": [
        "\n",
        "\n",
        "# Заключение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7SYGrASDS2S"
      },
      "source": [
        "В этом тьюториале мы постарались разобраться с тем, как работать с семантическими векторными моделями и библиотекой **gensim**. Теперь вы можете:\n",
        "* использовать готовые модели векторной семантики,осуществлять простые операции над векторами слов.\n",
        "* осуществлять предобработку текстовых данных, что может пригодиться во многих задачах обработки естественного языка;\n",
        "* тренировать векторные семантические модели. Формат моделей совместим с моделями, представленными на веб-сервисе **RusVectōrēs**;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM7jcoLqyZut"
      },
      "source": [
        "## Что мы не затронули?\n",
        "\n",
        "\n",
        "\n",
        "*   Оценка качества моделей (как тут считать точность-полноту-F-меру). См например в [этой тетрадке](https://github.com/ancatmara/data-science-nlp/blob/master/2.%20Embeddings.ipynb) у Оксаны оценку качества на задачах оценки семантической близости и поиска аналогии (Москва Россия Берлин Германия).\n",
        "*   Применение для реальных задач классификации текстов. См. в [этой тетрадке](https://github.com/mannefedov/compling_nlp_hse_course/blob/master/2020/Embeddings.ipynb) Миши Нефедова.\n",
        "\n"
      ]
    }
  ]
}
