{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшение размерности --  задача из машинного обучения (изначально -- из статистики), где алгоритм \"собирает\" многочисленные признаки в высокоуровневые абстракции.\n",
    "\n",
    "**Где используется уменьшение размерности:**<br>\n",
    "- Рекомендательные системы\n",
    "- Риск-менеджмент\n",
    "- Красивые визуализации\n",
    "- Определение похожих текстов (тематическое моделирование)\n",
    "\n",
    "\n",
    "**Популярные алгоритмы:** <br>\n",
    "- Метод главных компонент (PCA)\n",
    "- Латентное размещение Дирихле (LDA)\n",
    "- Сингулярное разложение (SVD)\n",
    "- t-SNE, UMAP (для визуализации)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Как работает объединение признаков в абстракцию:**<br>\n",
    " - CV: \"собака с треугольными ушами\" + \"с длинным носом\" + \"с большим хвостом\" >> овчарка\n",
    " - NLP:  текст с фразами \"нарезать кубиками\", \"200 грамм\", \"при температуре\" >> кулинарный рецепт\n",
    " - RecSys: \"пользователь слушает Alice Coltrane\" + \"лайкнул альбом Луи Армстронга\" >> любитель джаза\n",
    "\n",
    "! *важно отметить, что не все абстракции хорошо интерпретируемы (тк мы работаем с многомерными пространствами)*\n",
    "\n",
    "Уменьшение размерности хорошо работает для определения тематик текстов (Topic Modelling). Идея такая же: документ с текстом представляют как некоторую абстракцию из более низкоуровневых признаков. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сегодня будем делать топик-моделлинг на [корпусе новостей](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)\n",
    "\n",
    "наши алгоритмы --- LDA (Latent Dirichlet Allocation), SVD (Single Value Decomosition) и NMF (Non-negative Matrix Factorization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "<img src=\"https://lh4.googleusercontent.com/AGwwstY_3RaSvOzI4qYCmeotnnNw7pdQpHCEHg7Mz226GKJC_gp0veP-12cc8DsqY12mSroIir62uWvSIS06WTfDkb-WrOJZV24Dlfg4jlGOkqKGbpmO3NHr2g6IjgZr_rA9Z-E\" alt=\"\" class=\"lazyloaded\" data-ll-status=\"loaded\">\n",
    "\n",
    "[ссылка на картинку](https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/#:~:text=LDA%20Algorithm,by%20a%20statistical%20generative%20process.&text=In%20the%20process%20of%20generating,the%20multinomial%20topic%2Dword%20distributions.)\n",
    "\n",
    "- Корпус -- это коллекция из D документов.\n",
    "\n",
    "- Документ состоит из N слов.\n",
    "\n",
    "- В одном документе может встретиться K тем.\n",
    "\n",
    "Слова в корпусе -- единственная явная переменная. Скрытые (латентные) переменные - это распределение тем в корпусе и распределение слов в документе. Задача алгоритма LDA -- используя наблюдаемые слова извлечь информацию о структуре тем в корпусе.\n",
    "\n",
    "Для LDA нужны две матрицы: *“темы x слова\"* и  *“документы x темы”*. \n",
    "Они получаются из матрицы \"документы x слова\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # viz\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer # векторизация текстов\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation # dimred\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups # data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зададим несколько переменных, будем использовать их как параметры функций\n",
    "\n",
    "n_samples = 2000 # размер корпуса\n",
    "n_features = 1000 # максимальное количество слов в матрице \"слово x документ\" (= top1000 частотных их всех в корпусе)\n",
    "n_components = 10 # число тем в корпусе\n",
    "n_top_words = 20 # порог частотности для визуализаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала загрузим датасет и возьмем оттуда только текстовую часть (без заголовков, сносок и тд)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если возникает ошибка загрузки, раскомментьте ячейки ниже\n",
    "# import ssl\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "data, _ = fetch_20newsgroups(remove=('headers', 'footers','quotes'),\n",
    "                             return_X_y=True) # y нам не нужен, на самом деле "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для работы возьмем часть датасета\n",
    "data_samples = data[:n_samples] \n",
    "\n",
    "print(\"Общий датасет: {} документов;\\nФрагмент для работы: {} документов\".format(len(data),len(data_samples)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь документы надо векторизовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=1, # игнорируем слова,которые только в 1 доке или в 95% документов.\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tf.shape) # матрица \"слова x документы\"\n",
    "\n",
    "tf_vectorizer.get_feature_names()[100:107]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь отдадим эту матрицу алгоритму снижения размерности\n",
    "\n",
    "параметры в скобках: \n",
    "- n_components: число тем в корпусе\n",
    "-  max_iter: количество итераций алгоритма\n",
    "- learning_offset: параметр, который занижает значение ранних итераций (тк более важная часть обучения случается на поздних), обычно устанавливается больше чем 1 \n",
    "\n",
    "[про модель](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#examples-using-sklearn-decomposition-latentdirichletallocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=20,learning_offset=50)\n",
    "\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "используем функцию для упрощения визуализации топа частотности по темам:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15)) # параметры отображения \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1] \n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, 'Распределение по темам, LDA-модель')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "\n",
    "Уменьшение размерности с помощью метода SVD часто называется латентным семантическим анализом (LSA)\n",
    "\n",
    "[про модель](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем модель\n",
    "lsa_model = TruncatedSVD(n_components=n_components)\n",
    "\n",
    "lsa_topic_matrix = lsa_model.fit_transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, 'Распределение по темам, SVD-модель')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  NMF\n",
    "NMF -- альтернативный способ разложения матрицы, который подразумевает, что данные не-негативные (т.е. >=0). \n",
    "NMF часто заменяет PCA. При разложении изначальная матрица превращается в две, при этом оптимизируются два параметра: расстояние между матрицами и их произведение.\n",
    "\n",
    "\n",
    "[про модель](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для NMF понадобится tf-idf-векторизация, тк tf-idf не бывает негативным\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем первую модель\n",
    "\n",
    "nmf = NMF(n_components=n_components).fit(tfidf)\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "plot_top_words(nmf, tfidf_feature_names, n_top_words,\n",
    "               'Распределение по темам, NMF-модель (Frobenius norm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для второй версии добавим параметров:\n",
    "\n",
    "- beta-loss : мера оптимизации расстояния между матрицами (дивергенции)\n",
    "- solver: еще один параметр оптимизации, для KL-дивергенции нужен Multiplicative Update ('mu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучаем вторую модель\n",
    "\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu').fit(tfidf)\n",
    "\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "plot_top_words(nmf, tfidf_feature_names, n_top_words,\n",
    "               'Распределение по темам, NMF-модель(generalized KL-divergence)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
